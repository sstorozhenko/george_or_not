{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инструкция по запуску\n",
    "* Запускать все ячейки поочередно\n",
    "* Ячейки с обучением долго работают, поэтому оставила результаты в аутпуте. Я предупрежу, когда начнутся долгие ячейки :) \n",
    "* Для отображения картинок нужно поменять переменную **show_images** на True в ячейке ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image \n",
    "import random\n",
    "import IPython.display as Disp\n",
    "\n",
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GroupKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geog = pd.read_csv('georges.csv', header=None)\n",
    "df_notgeog = pd.read_csv('non_georges.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geog_ = df_geog.copy()\n",
    "df_geog_['target'] = 1\n",
    "df_notgeog_ = df_notgeog.copy()\n",
    "df_notgeog_['target'] = 0\n",
    "\n",
    "\n",
    "df_full = pd.concat([df_geog_, df_notgeog_], ignore_index=True)\n",
    "df_full.rename(columns={0:'url', 'target': 'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.pinimg.com/736x/17/0d/5b/170d5b93d80...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.pinimg.com/736x/47/b9/9a/47b99a2ddcd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.pinimg.com/736x/90/e8/90/90e890f054b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.pinimg.com/736x/0a/71/6f/0a716f6f14e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.pinimg.com/736x/f1/95/be/f195bea0b78...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  target\n",
       "0  https://i.pinimg.com/736x/17/0d/5b/170d5b93d80...       1\n",
       "1  https://i.pinimg.com/736x/47/b9/9a/47b99a2ddcd...       1\n",
       "2  https://i.pinimg.com/736x/90/e8/90/90e890f054b...       1\n",
       "3  https://i.pinimg.com/736x/0a/71/6f/0a716f6f14e...       1\n",
       "4  https://i.pinimg.com/736x/f1/95/be/f195bea0b78...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6047, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_full.head())\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разведывательный анализ :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Что мы вообще загрузили"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.pinimg.com/736x/17/0d/5b/170d5b93d80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.pinimg.com/736x/47/b9/9a/47b99a2ddcd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.pinimg.com/736x/90/e8/90/90e890f054b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.pinimg.com/736x/0a/71/6f/0a716f6f14e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.pinimg.com/736x/f1/95/be/f195bea0b78...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  https://i.pinimg.com/736x/17/0d/5b/170d5b93d80...\n",
       "1  https://i.pinimg.com/736x/47/b9/9a/47b99a2ddcd...\n",
       "2  https://i.pinimg.com/736x/90/e8/90/90e890f054b...\n",
       "3  https://i.pinimg.com/736x/0a/71/6f/0a716f6f14e...\n",
       "4  https://i.pinimg.com/736x/f1/95/be/f195bea0b78..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((2681, 1), (3366, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_geog.head()) \n",
    "\n",
    "df_geog.shape, df_notgeog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.pinimg.com/736x/17/0d/5b/170d5b93d80d247be60f22ca1216bef7.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geog.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательные функции для загрузки изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url): \n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200 and 'jpeg' in res.headers['content-type']:\n",
    "        img_arr = np.array(Image.open(BytesIO(res.content)))\n",
    "        return  img_arr\n",
    "    else: \n",
    "        return  np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_load_image(res, i, save_to_file=False, folder=''): \n",
    "    img = Image.open(BytesIO(await res.content.read()))\n",
    "    \n",
    "    if save_to_file:\n",
    "        img.save(folder + 'img' + str(i) + '.jpeg')    \n",
    "        \n",
    "    img_arr = np.array(img)\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(load_image(df_geog.iloc[0,0]))\n",
    "plt.show()\n",
    "plt.imshow(load_image(df_notgeog.iloc[0,0]))\n",
    "plt.show()\n",
    "\n",
    "if not show_images:\n",
    "    Disp.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ага, у нас тут картинки, и картинки разного размера. Посмотрим, какие размеры вообще представлены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ содержимого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_img(session, url, i, save_to_file):\n",
    "    async with session.get(url) as response:\n",
    "        if response.status != 200:\n",
    "            response.raise_for_status()\n",
    "        return await async_load_image(response, i, save_to_file)\n",
    "\n",
    "async def fetch_img_all(session, urls, save_to_file):\n",
    "    tasks = []\n",
    "    for i, url in enumerate(urls):\n",
    "        task = asyncio.create_task(fetch_img(session, url, i, save_to_file))\n",
    "        tasks.append(task)\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "async def loader_main(urls, save_to_file):    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        htmls = await fetch_img_all(session, urls, save_to_file)\n",
    "#         print(htmls)\n",
    "    return htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Seconds elapsed:  78.05\n"
     ]
    }
   ],
   "source": [
    "print('Processing...')\n",
    "start = time.time()\n",
    "\n",
    "urls = list(df_full['url'])\n",
    "img_list = await loader_main(urls, save_to_file=False)\n",
    "\n",
    "print('Seconds elapsed: ', round(time.time() - start, 2))\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не цветные:  46 \n",
      "С альфа-каналом:  3\n"
     ]
    }
   ],
   "source": [
    "not_colorful = [ind for ind, img in enumerate(img_list) if len(img.shape) < 3]\n",
    "alpha_ch = [ind for ind, img in enumerate(img_list) if (len(img.shape) == 3)&(img.shape[-1] > 3)]\n",
    "\n",
    "print('Не цветные: ', len(not_colorful), '\\nС альфа-каналом: ', len(alpha_ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_list[not_colorful[0]])\n",
    "plt.show()\n",
    "plt.imshow(img_list[alpha_ch[0]])\n",
    "plt.show()\n",
    "if not show_images:\n",
    "    Disp.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, это нам сейчас не нужно, мы хотим оценить размеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['hight'] = np.array([i.shape[0] for i in img_list])\n",
    "df_full['width'] = np.array([i.shape[1] for i in img_list])\n",
    "df_full['relation'] = df_full['width']/df_full['hight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>hight</th>\n",
       "      <th>width</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6047.000000</td>\n",
       "      <td>6047.000000</td>\n",
       "      <td>6047.000000</td>\n",
       "      <td>6047.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.443360</td>\n",
       "      <td>732.870184</td>\n",
       "      <td>549.738713</td>\n",
       "      <td>0.801997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496823</td>\n",
       "      <td>290.386326</td>\n",
       "      <td>165.446069</td>\n",
       "      <td>0.271161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.127138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>935.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5789.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>4.515337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target        hight        width     relation\n",
       "count  6047.000000  6047.000000  6047.000000  6047.000000\n",
       "mean      0.443360   732.870184   549.738713     0.801997\n",
       "std       0.496823   290.386326   165.446069     0.271161\n",
       "min       0.000000   110.000000   120.000000     0.127138\n",
       "25%       0.000000   538.000000   422.000000     0.666667\n",
       "50%       0.000000   700.000000   564.000000     0.736000\n",
       "75%       1.000000   935.000000   736.000000     0.833333\n",
       "max       1.000000  5789.000000   736.000000     4.515337"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_full.describe())\n",
    "df_full.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим странные случаи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_image_index = df_full[df_full['relation'] > 4].index[0]\n",
    "plt.imshow(img_list[strange_image_index])\n",
    "plt.title(str(strange_image_index))\n",
    "plt.show()\n",
    "if not show_images:\n",
    "    Disp.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_image_index = df_full[df_full['hight'] == 5789].index[0]\n",
    "plt.imshow(img_list[strange_image_index])\n",
    "plt.title(str(strange_image_index))\n",
    "plt.show()\n",
    "if not show_images:\n",
    "    Disp.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы для трансформации и классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformer(TransformerMixin, ABC):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        new_X = []\n",
    "        for x in X:\n",
    "            new_X.append(self.transform_one(x, y))\n",
    "        \n",
    "        return new_X\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BlurImageTransformer(ImageTransformer):\n",
    "    def __init__(self, blur_method=cv2.GaussianBlur, blur_kernel=(5, 5) ):\n",
    "        self.blur_method = blur_method\n",
    "        self.blur_kernel = blur_kernel\n",
    "      \n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        return self.blur_method(X, self.blur_kernel)\n",
    "    \n",
    "    \n",
    "class SegmetationImageTransformer(ImageTransformer):\n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        gray = cv2.cvtColor(X, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        ret, thresh = cv2.threshold(gray, 0, 255, \n",
    "                                    cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, \n",
    "                                   kernel, iterations=2)\n",
    "        sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "        \n",
    "        dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "        ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "        sure_fg = np.uint8(sure_fg)\n",
    "        \n",
    "        unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "        \n",
    "        ret, markers = cv2.connectedComponents(sure_fg)\n",
    "        \n",
    "        markers = markers + 1\n",
    "        markers[unknown == 255] = 0\n",
    "\n",
    "        markers = cv2.watershed(X, markers)\n",
    "        X[markers == -1] = [0, 0, 0]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "class ReshapeImageTransformer(ImageTransformer):\n",
    "    def __init__(self, chosen_shape=(150, 150)):\n",
    "        self.chosen_shape = chosen_shape\n",
    "    \n",
    "    def transform_one(self, X, y=None, **fit_params): \n",
    "        X = cv2.resize(X, self.chosen_shape)\n",
    "       \n",
    "        return X\n",
    "    \n",
    "\n",
    "class FilterImageTransformer(ImageTransformer):\n",
    "    def __init__(self, filter_arr=def_filter_arr):\n",
    "        self.filter_arr = filter_arr\n",
    "    \n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        X = cv2.filter2D(X, -1, self.filter_arr)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "class GrayImageTransformer(ImageTransformer):\n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        X = cv2.cvtColor(X, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        return X\n",
    "    \n",
    "class ScalerImageTransformer(ImageTransformer):\n",
    "    def __init__(self, is_flatten=True):\n",
    "        self.is_flatten = is_flatten\n",
    "        \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        if self.is_flatten:\n",
    "            X = np.array([i.flatten() for i in X])\n",
    "        else:\n",
    "            X = np.array([i for i in X])\n",
    "\n",
    "        X = X/255\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class FlipperImageTransformer(ImageTransformer):\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        res_0 = []\n",
    "        res_1 = []\n",
    "        for x in X:\n",
    "            res_0.append(cv2.flip(x, 0))\n",
    "            res_1.append(cv2.flip(x, 1))\n",
    "            \n",
    "        X.extend(res_0)\n",
    "        X.extend(res_1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def transform_one(self, X, y=None, **fit_params):\n",
    "        pass\n",
    "    \n",
    "class TransformerCombiner:\n",
    "    @classmethod\n",
    "    def process_step(cls, X, y, step, verbose, i, n):\n",
    "        if verbose:\n",
    "            print(f'Transformation {i} of {n}, step {step[0]}')\n",
    "            \n",
    "        return step[1].fit_transform(X, y)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def fit_transform(cls, X, y, steps, verbose=False):\n",
    "        for i, step in enumerate(steps):\n",
    "            X = cls.process_step(X, y, step, verbose, i, len(steps))\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_shape, kernel_size=3, out_channel=6, *args):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_shape[0], out_channel, kernel_size)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        n_inp = self.get_linear_layer_inp_shape(input_shape, kernel_size, out_channel)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_inp, 64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, self.get_num_features(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = torch.flatten(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def get_num_features(self, x):\n",
    "        size = x.size()[1:] \n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    \n",
    "    def get_linear_layer_inp_shape(self, input_shape, kernel_size, out_channel):\n",
    "        print()\n",
    "        return (input_shape[1]//kernel_size - 1) * (input_shape[2]//kernel_size - 1) * out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для поддержки интерфейса scikit-learn\n",
    "class CNNClassifier(BaseEstimator):\n",
    "    def __init__(self, input_shape, n_iter, n_epoch, criterion, sens=0.5, \n",
    "                 batch_size=4, kernel_size=3, out_channel=6):\n",
    "        \n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channel = out_channel\n",
    "        \n",
    "        self.net = ConvNet(self.input_shape, self.kernel_size, self.out_channel)\n",
    "        \n",
    "        self.n_iter = n_iter\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        self.sens = sens\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def transform_input_data(self, X, y=None):\n",
    "        inputs = torch.tensor(X) \n",
    "        \n",
    "        if not (y is None):\n",
    "            labels = torch.tensor(y)\n",
    "        else:\n",
    "            labels = None\n",
    "            \n",
    "        inputs = inputs.unsqueeze(1).float()\n",
    "        \n",
    "        return inputs, labels\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        inputs, labels = self.transform_input_data(X, y)\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            \n",
    "            permutation = torch.randperm(inputs.size()[0])\n",
    "            \n",
    "            for i in range(0, inputs.size()[0], self.batch_size):\n",
    "                indices = permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_y = inputs[indices], labels[indices]\n",
    "\n",
    "\n",
    "                for k in range(self.n_iter):\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.net(batch_x)\n",
    "                    loss = self.criterion(outputs.float(), batch_y.float())\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # print statistics\n",
    "                    loss = loss.item()\n",
    "                    \n",
    "                if i % 1000 == 0: \n",
    "                    outputs_ = self.net(inputs)\n",
    "                    y_pred = np.where(outputs_ > self.sens, 1, 0)\n",
    "                    accuracy = accuracy_score(y_pred, y)\n",
    "                    print(f'Epoch: {epoch}, iter: {i}, accuracy: {accuracy}')\n",
    "\n",
    "        print('Finished Training')\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inputs, _ = self.transform_input_data(X)\n",
    "        outputs = self.net(inputs)\n",
    "        \n",
    "        return np.where(outputs > self.sens, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Транформация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_shape = (51, 51)\n",
    "blur_kernel = (3, 3)\n",
    "def_filter_arr = np.array(\n",
    "    [\n",
    "        [-1, -2, -1],\n",
    "        [0, 0, 0],\n",
    "        [1, 2, 1]\n",
    "    ]\n",
    ")\n",
    "george_max_index = max(df_full[df_full['target'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurer = BlurImageTransformer(blur_method=cv2.GaussianBlur, blur_kernel=(5, 5) )\n",
    "segmetator = SegmetationImageTransformer()\n",
    "reshaper = ReshapeImageTransformer(chosen_shape=chosen_shape)\n",
    "filterer = FilterImageTransformer(filter_arr=def_filter_arr)\n",
    "grayer = GrayImageTransformer()\n",
    "flipper = FlipperImageTransformer()\n",
    "scaler = ScalerImageTransformer(is_flatten=True)\n",
    "scaler_t = ScalerImageTransformer(is_flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1 if i <= george_max_index else 0 for i in range(df_full.shape[0])])\n",
    "colorful_img_list = [cv2.cvtColor(i,cv2.COLOR_GRAY2RGB) if len(i.shape) < 3 else \n",
    "                     (cv2.cvtColor(i,cv2.COLOR_BGRA2BGR) if i.shape[2] > 3 else i) \n",
    "                     for i in img_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала рассматриваем базовую трансформацию:\n",
    "* Приводим к одному размеру\n",
    "* Оставляем один канал (оттенки серого)\n",
    "* Нормируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation 0 of 3, step reshaper\n",
      "Transformation 1 of 3, step grayer\n",
      "Transformation 2 of 3, step scaler\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6047, 2601)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = TransformerCombiner.fit_transform(colorful_img_list, \n",
    "                                       y, \n",
    "                                       [('reshaper', reshaper), \n",
    "                                        ('grayer', grayer), ('scaler', scaler)],\n",
    "                                       True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation 0 of 3, step reshaper\n",
      "Transformation 1 of 3, step grayer\n",
      "Transformation 2 of 3, step scaler\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6047, 51, 51)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = TransformerCombiner.fit_transform(colorful_img_list, y, \n",
    "                                       [('reshaper', reshaper), \n",
    "                                        ('grayer', grayer), ('scaler', scaler_t)],\n",
    "                                       True)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас пока нет зависимости между объектами (отраженных изображений или вырезенных элементов), значит мы можем произвольно разделить их на dev-выборку и валидационную (1/8 всех объектов).\n",
    "\n",
    "Dev-выборку будем использовать для настройки параметров, а валидационную - для оценки и выбора модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5291, 2601), (5291,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_kfold = StratifiedKFold(n_splits=8, random_state=1)\n",
    "\n",
    "cv = strat_kfold.split(X_, y)\n",
    "dev_ind, val_ind = next(cv)\n",
    "\n",
    "X_dev = X[dev_ind]\n",
    "X_dev_t = X_t[dev_ind]\n",
    "X_val = X[val_ind]\n",
    "X_val_t = X_t[val_ind]\n",
    "\n",
    "y_dev = y[dev_ind]\n",
    "y_val = y[val_ind]\n",
    "\n",
    "X_dev.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка гиперпараметров у моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВНИМАНИЕ! Начинаются долкие ячейки :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала найдем оптимальные гиперпараметры для всех кандидатов в классификаторы. Т.к. нейронная сеть обучается долго, то для нее я не буду настраивать гипер-параметры. Но возможность этого оставила :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_grid_list = {\n",
    "    'LogReg': ( LogisticRegression(), {\n",
    "       'max_iter': [5000, 10000],\n",
    "       'C':np.logspace(-3,3,7)\n",
    "    } ),\n",
    "    'SVM': ( SVC(), {\n",
    "        'C': [1, 10, 100, 1000], \n",
    "        'gamma': [0.001, 0.0001, 'auto'], \n",
    "        'kernel': ['rbf']\n",
    "    } ),\n",
    "    'kN': ( KNeighborsClassifier(), {\n",
    "        'n_neighbors': [1, 2, 3],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    } ),\n",
    "    'MLP': ( MLPClassifier(max_iter=5000), {\n",
    "        'hidden_layer_sizes': [(100,), (100, 100), (20, 40, 20)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'alpha': [0.005, 0.1]\n",
    "    } ),\n",
    "    'cNN': ( CNNClassifier((1, *(X_dev_t.shape[1:])), 1000, 4, nn.MSELoss(), batch_size=10), {\n",
    "        'batch_size': [200]\n",
    "    } )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_assess_models(X, y, model_grid_list, cv_split=3, models_exclude=[]):\n",
    "    gs_dict = {}\n",
    "    \n",
    "    for model_name, model in model_grid_list.items():\n",
    "        \n",
    "        if model_name not in models_exclude:\n",
    "            print(f'Processing {model_name}')\n",
    "            \n",
    "            strat_kfold = StratifiedKFold(n_splits=cv_split, shuffle=True, random_state=2)\n",
    "            cv = strat_kfold.split(X, y)\n",
    "            gs_dict[model_name] = GridSearchCV(*(model), verbose=True, cv=cv, scoring='accuracy') \n",
    "            gs_dict[model_name].fit(X, y)\n",
    "            \n",
    "            print(f'Model {model_name}, score: {gs_dict[model_name].best_score_}')\n",
    "        else:\n",
    "            print(f'Model {model_name} is excluded!')\n",
    "            \n",
    "    return gs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LogReg\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
      "Model LogReg, score: 0.6773768686903766\n",
      "Processing SVM\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Model SVM, score: 0.7187663052868466\n",
      "Processing kN\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Model kN, score: 0.6924933835637992\n",
      "Processing MLP\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Model MLP, score: 0.7233026316974133\n",
      "Model cNN is excluded!\n"
     ]
    }
   ],
   "source": [
    "gs_dict = tune_and_assess_models(X_dev, y_dev, model_grid_list, models_exclude=['cNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogReg is excluded!\n",
      "Model SVM is excluded!\n",
      "Model kN is excluded!\n",
      "Model MLP is excluded!\n",
      "Processing cNN\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "Epoch: 0, iter: 0, accuracy: 0.5817975616671393\n",
      "Epoch: 0, iter: 1000, accuracy: 0.7258293166997448\n",
      "Epoch: 0, iter: 2000, accuracy: 0.7692089594556281\n",
      "Epoch: 0, iter: 3000, accuracy: 0.8134391834420187\n",
      "Epoch: 1, iter: 0, accuracy: 0.8219449957470939\n",
      "Epoch: 1, iter: 1000, accuracy: 0.8403742557414233\n",
      "Epoch: 1, iter: 2000, accuracy: 0.8627728948114545\n",
      "Epoch: 1, iter: 3000, accuracy: 0.8962290898780834\n",
      "Epoch: 2, iter: 0, accuracy: 0.905585483413666\n",
      "Epoch: 2, iter: 1000, accuracy: 0.9254323787921747\n",
      "Epoch: 2, iter: 2000, accuracy: 0.9325205557130706\n",
      "Epoch: 2, iter: 3000, accuracy: 0.9506662886305642\n",
      "Epoch: 3, iter: 0, accuracy: 0.9614403175503261\n",
      "Epoch: 3, iter: 1000, accuracy: 0.9634250070881769\n",
      "Epoch: 3, iter: 2000, accuracy: 0.9662602778565353\n",
      "Epoch: 3, iter: 3000, accuracy: 0.9744825630847745\n",
      "Finished Training\n",
      "\n",
      "Epoch: 0, iter: 0, accuracy: 0.5562801247519138\n",
      "Epoch: 0, iter: 1000, accuracy: 0.6648709951800397\n",
      "Epoch: 0, iter: 2000, accuracy: 0.7161893960873263\n",
      "Epoch: 0, iter: 3000, accuracy: 0.7533314431528211\n",
      "Epoch: 1, iter: 0, accuracy: 0.7760136092996881\n",
      "Epoch: 1, iter: 1000, accuracy: 0.8015310462149136\n",
      "Epoch: 1, iter: 2000, accuracy: 0.8250637935922881\n",
      "Epoch: 1, iter: 3000, accuracy: 0.8400907286645874\n",
      "Epoch: 2, iter: 0, accuracy: 0.8769492486532464\n",
      "Epoch: 2, iter: 1000, accuracy: 0.8914091295718741\n",
      "Epoch: 2, iter: 2000, accuracy: 0.9075701729515169\n",
      "Epoch: 2, iter: 3000, accuracy: 0.9180606747944429\n",
      "Epoch: 3, iter: 0, accuracy: 0.9311029203288914\n",
      "Epoch: 3, iter: 1000, accuracy: 0.9432945846328324\n",
      "Epoch: 3, iter: 2000, accuracy: 0.9518003969379075\n",
      "Epoch: 3, iter: 3000, accuracy: 0.954635667706266\n",
      "Finished Training\n",
      "\n",
      "Epoch: 0, iter: 0, accuracy: 0.5566893424036281\n",
      "Epoch: 0, iter: 1000, accuracy: 0.6678004535147393\n",
      "Epoch: 0, iter: 2000, accuracy: 0.7063492063492064\n",
      "Epoch: 0, iter: 3000, accuracy: 0.7474489795918368\n",
      "Epoch: 1, iter: 0, accuracy: 0.7757936507936508\n",
      "Epoch: 1, iter: 1000, accuracy: 0.7896825396825397\n",
      "Epoch: 1, iter: 2000, accuracy: 0.798469387755102\n",
      "Epoch: 1, iter: 3000, accuracy: 0.814625850340136\n",
      "Epoch: 2, iter: 0, accuracy: 0.8302154195011338\n",
      "Epoch: 2, iter: 1000, accuracy: 0.8591269841269841\n",
      "Epoch: 2, iter: 2000, accuracy: 0.8633786848072562\n",
      "Epoch: 2, iter: 3000, accuracy: 0.905045351473923\n",
      "Epoch: 3, iter: 0, accuracy: 0.8999433106575964\n",
      "Epoch: 3, iter: 1000, accuracy: 0.921485260770975\n",
      "Epoch: 3, iter: 2000, accuracy: 0.921201814058957\n",
      "Epoch: 3, iter: 3000, accuracy: 0.9175170068027211\n",
      "Finished Training\n",
      "\n",
      "\n",
      "Epoch: 0, iter: 0, accuracy: 0.573048573048573\n",
      "Epoch: 0, iter: 1000, accuracy: 0.6475146475146475\n",
      "Epoch: 0, iter: 2000, accuracy: 0.7011907011907011\n",
      "Epoch: 0, iter: 3000, accuracy: 0.7323757323757324\n",
      "Epoch: 0, iter: 4000, accuracy: 0.7382347382347383\n",
      "Epoch: 0, iter: 5000, accuracy: 0.7633717633717634\n",
      "Epoch: 1, iter: 0, accuracy: 0.7612927612927612\n",
      "Epoch: 1, iter: 1000, accuracy: 0.7769797769797769\n",
      "Epoch: 1, iter: 2000, accuracy: 0.8013608013608013\n",
      "Epoch: 1, iter: 3000, accuracy: 0.7985257985257985\n",
      "Epoch: 1, iter: 4000, accuracy: 0.8285768285768286\n",
      "Epoch: 1, iter: 5000, accuracy: 0.8421848421848421\n",
      "Epoch: 2, iter: 0, accuracy: 0.83991683991684\n",
      "Epoch: 2, iter: 1000, accuracy: 0.8552258552258553\n",
      "Epoch: 2, iter: 2000, accuracy: 0.8671328671328671\n",
      "Epoch: 2, iter: 3000, accuracy: 0.8711018711018711\n",
      "Epoch: 2, iter: 4000, accuracy: 0.883953883953884\n",
      "Epoch: 2, iter: 5000, accuracy: 0.8917028917028917\n",
      "Epoch: 3, iter: 0, accuracy: 0.9068229068229068\n",
      "Epoch: 3, iter: 1000, accuracy: 0.9113589113589113\n",
      "Epoch: 3, iter: 2000, accuracy: 0.9081459081459081\n",
      "Epoch: 3, iter: 3000, accuracy: 0.9077679077679077\n",
      "Epoch: 3, iter: 4000, accuracy: 0.9262899262899262\n",
      "Epoch: 3, iter: 5000, accuracy: 0.9321489321489321\n",
      "Finished Training\n",
      "Model cNN, score: 0.7507070894154598\n"
     ]
    }
   ],
   "source": [
    "gs_dict_cnn = tune_and_assess_models(X_dev_t, y_dev, model_grid_list, \n",
    "                                     models_exclude=[i for i in model_grid_list.keys() if i != 'cNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На кросс-валидации сверточная нейронная сеть показывает наилучший результат (без настройки гипер-параметров)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть вероятность, что использование дополнительной подготовки данных улучшит результат nonDL модели. Попробуем выбрать наилучший пайплайн для подготовки данных для каждой из них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(gs_result):\n",
    "    best_estimator = None\n",
    "    best_score = 0\n",
    "    for model_name, gs in gs_result.items():\n",
    "        if gs.best_score_ > best_score:\n",
    "            best_estimator = gs.best_estimator_\n",
    "            best_score = gs.best_score_\n",
    "            best_model = model_name\n",
    "            \n",
    "    return best_estimator, best_score, best_model\n",
    "\n",
    "\n",
    "def get_preprocessing_pipeline_score(estimator, pipeline_list, img_list, y, cv_split=3):\n",
    "    \n",
    "    n_objects = len(img_list)\n",
    "    \n",
    "    acc = {}\n",
    "    for pl_name, pl in pipeline_list.items():\n",
    "        print(f'Processing {pl_name}...')\n",
    "        X_ = TransformerCombiner.fit_transform(img_list, y, pl)\n",
    "        y_ = np.array(list(y)*(X_.shape[0]//n_objects))\n",
    "        \n",
    "        group_kfold = GroupKFold(n_splits=cv_split)\n",
    "        groups = np.array([i % (n_objects) for i in range(X_.shape[0])])\n",
    "\n",
    "        cv = group_kfold.split(X_, y_, groups)\n",
    "        \n",
    "        acc[pl_name] = cross_val_score(estimator, X_, y_, cv=cv, \n",
    "                                   scoring='accuracy')\n",
    "        \n",
    "        \n",
    "        print(f'Pipeline {pl_name}, score: {acc[pl_name].mean()}')\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_dict = {\n",
    "    'baseline': [('reshaper', reshaper), ('grayer', grayer), ('scaler', scaler)],\n",
    "    'simple': [('reshaper', reshaper), ('grayer', grayer), ('flipper', flipper), ('scaler', scaler)],\n",
    "    'filter': [('reshaper', reshaper), ('filter', filterer), ('grayer', grayer), ('flipper', flipper), \n",
    "               ('scaler', scaler)],\n",
    "    'segmentation': [('reshaper', reshaper), ('segmetator', segmetator), \n",
    "                     ('grayer', grayer), ('flipper', flipper), ('scaler', scaler)],\n",
    "    'segm+filter': [('reshaper', reshaper), ('filter', filterer), ('segmetator', segmetator), \n",
    "                     ('grayer', grayer), ('flipper', flipper), ('scaler', scaler)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogReg\n",
      "Processing baseline...\n",
      "Pipeline baseline, score: 0.6788902994234816\n",
      "Processing simple...\n",
      "Pipeline simple, score: 0.6543209519264944\n",
      "Processing filter...\n",
      "Pipeline filter, score: 0.6448067031690724\n",
      "Processing segmentation...\n",
      "Pipeline segmentation, score: 0.6422877484852474\n",
      "Processing segm+filter...\n",
      "Pipeline segm+filter, score: 0.6366800517396093\n",
      "Model LogReg comleted\n",
      "\n",
      "Model SVM\n",
      "Processing baseline...\n",
      "Pipeline baseline, score: 0.7168800904114088\n",
      "Processing simple...\n",
      "Pipeline simple, score: 0.7129752890631265\n",
      "Processing filter...\n",
      "Pipeline filter, score: 0.7100755472038188\n",
      "Processing segmentation...\n",
      "Pipeline segmentation, score: 0.6940116732813165\n",
      "Processing segm+filter...\n",
      "Pipeline segm+filter, score: 0.683993483530258\n",
      "Model SVM comleted\n",
      "\n",
      "Model kN\n",
      "Processing baseline...\n",
      "Pipeline baseline, score: 0.7064844719005646\n",
      "Processing simple...\n",
      "Pipeline simple, score: 0.6885284880112419\n",
      "Processing filter...\n",
      "Pipeline filter, score: 0.6771226652043981\n",
      "Processing segmentation...\n",
      "Pipeline segmentation, score: 0.6860088902265388\n",
      "Processing segm+filter...\n",
      "Pipeline segm+filter, score: 0.6466261784644952\n",
      "Model kN comleted\n",
      "\n",
      "Model MLP\n",
      "Processing baseline...\n",
      "Pipeline baseline, score: 0.6970318536439596\n",
      "Processing simple...\n",
      "Pipeline simple, score: 0.6966531744102443\n",
      "Processing filter...\n",
      "Pipeline filter, score: 0.6815363737142092\n",
      "Processing segmentation...\n",
      "Pipeline segmentation, score: 0.6794595151562443\n",
      "Processing segm+filter...\n",
      "Pipeline segm+filter, score: 0.6568976070502148\n",
      "Model MLP comleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, gs in gs_dict.items():\n",
    "    print(f'Model {model_name}')\n",
    "    best_estimator = gs.best_estimator_\n",
    "    scoring = get_preprocessing_pipeline_score(\n",
    "                                        best_estimator,\n",
    "                                        steps_dict,\n",
    "                                        [img for i, img in enumerate(colorful_img_list) if i in dev_ind], \n",
    "                                        y_dev\n",
    "                                    )\n",
    "    print(f'Model {model_name} comleted\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все придуманные предобработки не показали себя хорошо на кросс-валидации. Поэтому для финальной модели будет использоваться бэйзлайновый пайплайн подготовки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка точности моделей и выбор лучшей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Примечание_: на самом деле, так как здесь я не настраивала параметры для сверточной нейронной сети, то риска переобучиться при настройке параметров у нас нет, а, так как даже без настройки гипер-параметров cNN показала себя лучше, чем классический модели и персептрон, то она является более предпочтительной. \n",
    "\n",
    "Однако, для честности, представим, что cNN мы тоже настраивали, а наш валидационный датасет достаточно велик, чтобы использовать его в качестве отложенной выборки для нешумной оценки качества моделей (что, конечно же, не так, когда речь идет про ~700 объектов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn стэк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LogReg\n",
      "Model LogReg, score 0.7261904761904762\n",
      "Processing SVM\n",
      "Model SVM, score 0.7738095238095238\n",
      "Processing kN\n",
      "Model kN, score 0.7579365079365079\n",
      "Processing MLP\n",
      "Model MLP, score 0.716931216931217\n"
     ]
    }
   ],
   "source": [
    "accuracy = {}\n",
    "for model_name, gs in gs_dict.items():\n",
    "    print(f'Processing {model_name}')\n",
    "    params = gs.best_params_\n",
    "    \n",
    "    best_estimator = model_grid_list[model_name][0].set_params(**params)\n",
    "    best_estimator.fit(X_dev, y_dev)\n",
    "    y_pred = best_estimator.predict(X_val)\n",
    "    \n",
    "    accuracy[model_name] = accuracy_score(y_val, y_pred)\n",
    "    print(f'Model {model_name}, score {accuracy[model_name]}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cNN\n",
      "Epoch: 0, iter: 0, accuracy: 0.5583065583065583\n",
      "Epoch: 0, iter: 1000, accuracy: 0.6633906633906634\n",
      "Epoch: 0, iter: 2000, accuracy: 0.6998676998676999\n",
      "Epoch: 0, iter: 3000, accuracy: 0.7233037233037233\n",
      "Epoch: 0, iter: 4000, accuracy: 0.7535437535437536\n",
      "Epoch: 0, iter: 5000, accuracy: 0.7611037611037611\n",
      "Epoch: 1, iter: 0, accuracy: 0.766017766017766\n",
      "Epoch: 1, iter: 1000, accuracy: 0.7852957852957853\n",
      "Epoch: 1, iter: 2000, accuracy: 0.8217728217728217\n",
      "Epoch: 1, iter: 3000, accuracy: 0.8223398223398224\n",
      "Epoch: 1, iter: 4000, accuracy: 0.8476658476658476\n",
      "Epoch: 1, iter: 5000, accuracy: 0.8365148365148365\n",
      "Epoch: 2, iter: 0, accuracy: 0.8597618597618598\n",
      "Epoch: 2, iter: 1000, accuracy: 0.879039879039879\n",
      "Epoch: 2, iter: 2000, accuracy: 0.8848988848988849\n",
      "Epoch: 2, iter: 3000, accuracy: 0.8862218862218862\n",
      "Epoch: 2, iter: 4000, accuracy: 0.9020979020979021\n",
      "Epoch: 2, iter: 5000, accuracy: 0.9124929124929125\n",
      "Epoch: 3, iter: 0, accuracy: 0.9276129276129276\n",
      "Epoch: 3, iter: 1000, accuracy: 0.9317709317709317\n",
      "Epoch: 3, iter: 2000, accuracy: 0.934038934038934\n",
      "Epoch: 3, iter: 3000, accuracy: 0.941031941031941\n",
      "Epoch: 3, iter: 4000, accuracy: 0.9417879417879418\n",
      "Epoch: 3, iter: 5000, accuracy: 0.9502929502929502\n",
      "Finished Training\n",
      "Model cNN, score 0.8108465608465608\n"
     ]
    }
   ],
   "source": [
    "for model_name, gs in gs_dict_cnn.items():\n",
    "    print(f'Processing {model_name}')\n",
    "    params = gs.best_params_\n",
    "    \n",
    "    best_estimator = model_grid_list[model_name][0].set_params(**params)\n",
    "    best_estimator.fit(X_dev_t, y_dev)\n",
    "    y_pred = best_estimator.predict(X_val_t)\n",
    "    \n",
    "    accuracy[model_name] = accuracy_score(y_val, y_pred)\n",
    "    print(f'Model {model_name}, score {accuracy[model_name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_best_of_the_best = model_grid_list['cNN'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты\n",
    "\n",
    "Лучшая модель - **сверточная нейронная сеть** (даже не смотря на маленькую выборку)\n",
    "\n",
    "Точность:\n",
    "* на кросс-валидации: 75.1%\n",
    "* на отложенной выборке: 81.1%\n",
    "\n",
    "\n",
    "Имеет место сильное переобучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
